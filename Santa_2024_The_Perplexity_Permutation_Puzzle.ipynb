{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQBPQDaAEzBd5Fxzi5ozC9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinal88/CprogrammingIntroduction/blob/main/Santa_2024_The_Perplexity_Permutation_Puzzle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3jKs061ZOTJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the data"
      ],
      "metadata": {
        "id": "skRXMIKXWHKT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTkOeXShV6w3",
        "outputId": "565d9bd7-f537-40ca-faa0-7c79dc97285c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                               text\n",
            "0   0  advent chimney elf family fireplace gingerbrea...\n",
            "1   1  advent chimney elf family fireplace gingerbrea...\n",
            "2   2  yuletide decorations gifts cheer holiday carol...\n",
            "3   3  yuletide decorations gifts cheer holiday carol...\n",
            "4   4  hohoho candle poinsettia snowglobe peppermint ...\n",
            "CPU times: user 13.2 ms, sys: 2.03 ms, total: 15.2 ms\n",
            "Wall time: 20 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import pandas as pd\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv(\"/content/sample_submission.csv\")  # Example path\n",
        "# test_df = pd.read_csv(\"test.csv\")\n",
        "# Display structure\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Model for Perplexity"
      ],
      "metadata": {
        "id": "-p5GAnDFXfuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "import numpy as np\n",
        "# Load model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def calculate_perplexity(sequence):\n",
        "  inputs = tokenizer(sequence, return_tensors=\"pt\", truncation = True)\n",
        "  # Converts the input text (sequence) into tokenized input IDs that GPT-2 can process.\n",
        "  # return_tensors=\"pt\": Ensures that the output is in PyTorch tensor format, which is required for the GPT-2 model.\n",
        "  # truncation=True: Truncates text that exceeds the model's maximum input length. (typically 1024 tokens)\n",
        "\n",
        "  outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "  # labels=inputs[\"input_ids\"]: Used to compute the loss between the model's predictions and the input tokens (language modeling loss).\n",
        "\n",
        "  loss = outputs.loss.item()\n",
        "  # Retrieves the scalar value of the loss (cross-entropy).\n",
        "  return np.exp(loss)\n",
        "  # Perplexity is the exponential of the cross-entropy loss, representing how \"confused\" the model is about predicting the text.\n",
        "\n",
        "#Example usage\n",
        "sentence = \"Santa loves coding puzzle durint Christmas.\"\n",
        "\n",
        "perplexity = calculate_perplexity(sentence)\n",
        "# Perplexity: a metric used in NLP to evaluate how well a language model predicts a sequence of text. A lower perplexity value indactes\n",
        "# better predictive performance by the model for the given input\n",
        "print(f\"Perplexity of the sentence '{sentence}': {perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiBlO-JQXeWr",
        "outputId": "618ad789-1326-4e3f-dd12-2a8817db9995"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of the sentence 'Santa loves coding puzzle durint Christmas.': 16148.121124604595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Programming for Sentence Reconstruction\n",
        "\n",
        "Use dynamic programming to build coherent sentences efficiently\n",
        "- Define a state as the current position in the word sequence\n",
        "- Define a transition cost as the perplexity of moving to the next word"
      ],
      "metadata": {
        "id": "dUN7bmIRc9Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def dp_min_perplexity(words):\n",
        "  n = len(words) # Number of words in the input list words\n",
        "  dp = np.full((n, n), float('inf')) # A 2D DP table dp is initialized with dimensions (n, n) filled with inf (infinity).\n",
        "  # This table will store the cumulative perplexities for different word sequences.\n",
        "  dp[0][0] = 0 # Starting point\n",
        "  for i in range(1, n):\n",
        "    for j in range(i):\n",
        "      sequence = \" \".join(words[:i])\n",
        "      dp[i][j] = calculate_perplexity(sequence) + dp[i-1][j]\n",
        "\n",
        "# Traceback for best sequence\n",
        "  best_path = []\n",
        "  cur = np.argmin(dp[-1])\n",
        "  for i in range(n-1, -1, -1):\n",
        "    best_path.append(words[cur])\n",
        "    cur = np.argmin(dp[i-1]) if i > 0 else 0\n",
        "  return \" \".join(best_path[::-1])\n",
        "\n",
        "# Example usage\n",
        "tokens = [ \"Santa\", \"loves\", \"coding\", \"puzzles\", \"during\", \"Christmas\"]\n",
        "best_sequence = dp_min_perplexity(tokens)\n",
        "print(\"Best Sequence:\", best_sequence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oEjCh0vc2gM",
        "outputId": "89fcc14e-d9a1-46cf-cfef-0d6c1ada3c56"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Sequence: Santa Santa Santa Santa Santa Santa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rearranaging Words"
      ],
      "metadata": {
        "id": "rcX4Q2rwtI6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def generate_permutation(tokens):\n",
        "  return list(itertools.permutations(tokens))\n",
        "\n",
        "# Generate and evaluate permutations\n",
        "tokens = [\"Santa\", \"loves\", \"coding\", \"puzzles\"]\n",
        "permutations = generate_permutation(tokens)\n",
        "\n",
        "# Score each permutation\n",
        "best_sequence = None\n",
        "lowest_perplexity = float('inf')\n",
        "\n",
        "for perm in permutations:\n",
        "  candidate = \" \".join(perm)\n",
        "  perplexity = calculate_perplexity(candidate)\n",
        "  if perplexity < lowest_perplexity:\n",
        "    lowest_perplexity = perplexity\n",
        "    best_sequence = candidate\n",
        "\n",
        "print(\"Best Sequence:\", best_sequence)\n",
        "print(\"Lowest Perplexity:\", lowest_perplexity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uOFLX6EtHU5",
        "outputId": "69afafc5-6ca3-4a5c-f24e-ffd56995b4ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Sequence: puzzles Santa loves coding\n",
            "Lowest Perplexity: 3797.983879804625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam Search (Scalable Optimization)\n"
      ],
      "metadata": {
        "id": "Oe8fsiEGuKLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "\n",
        "def beam_search(tokens, beam_width=3):\n",
        "  sequences = [{\"\", 1.0}] # Initialize beam with empty sequence\n",
        "  for token in tokens:\n",
        "    all_candidates = []\n",
        "    for seq, score in sequences:\n",
        "      new_seq = f\"{seq} {token}\".strip()\n",
        "      new_score = score * calculate_perplexity(new_seq)\n",
        "      all_candidates.append((new_seq, new_score))\n",
        "\n",
        "      # Keep top `beam_width` candidates\n",
        "    sequences = heapq.nsmallest(beam_width, all_candidates, key=lambda x: x[1])\n",
        "  return sequences[0] # Best sequence\n",
        "\n",
        "#Example usage\n",
        "tokens = [\"Santa\", \"loves\", \"coding\", \"puzzles\"]\n",
        "best_seq = beam_search(tokens)\n",
        "print(f\"Best sequence: {best_seq[0]} with perplexity {best_seq[1]}\")"
      ],
      "metadata": {
        "id": "UoQFSuFsuJAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb7134d-c66c-4a96-be37-a4e56f267448"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best sequence: Santa loves coding puzzles with perplexity nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UFCWg9hYPaeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dynamic programming for Sentence Reconstruction\n",
        "# Use dynamic programming to build coherent sentences efficiently\n",
        "#**Define a state as the current position in the word sequence\n",
        "#**Define a transition cost as the perplexity of moving to the next word\n",
        "\n",
        "%%time\n",
        "import numpy as np\n",
        "def dp_min_perplexity(words):\n",
        "  n = len(words)\n",
        "  dp = np.full((n, n), float('inf')) # DP Table\n",
        "  dp[0][0] = 0\n",
        "  for i in range(1, n):\n",
        "    for j in range(i):\n",
        "      sequence = \" \".join(words[:i])\n",
        "      dp[i][j] = calculate_perplexity(sequence) + dp[i-1][j]\n",
        "\n",
        "  # Traceback for best sequence\n",
        "  best_path = []\n",
        "  curr = np.argmin(dp[-1])\n",
        "  for i in range(n-1, -1, -1):\n",
        "    best_path.append(words[curr])\n",
        "    curr = np.argmin(dp[i-1]) if i > 0 else 0\n",
        "  return \" \".join(best_path[::-1])\n",
        "\n",
        "# Example usage\n",
        "tokens = [\"Santa\", \"loves\", \"coding\", \"puzzles\", \"during\", \"Christmas\"]\n",
        "best_sequence = dp_min_perplexity(tokens)\n",
        "print(\"Best Sequence:\", best_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_NQ27M0PYAU",
        "outputId": "bbc22e22-0745-4b73-ddba-7214633a37ef"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Sequence: Santa Santa Santa Santa Santa Santa\n",
            "CPU times: user 1.39 s, sys: 395 µs, total: 1.39 s\n",
            "Wall time: 1.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "predictions = []\n",
        "for i, row in train_df.iterrows():\n",
        "  scrambled_words = row[\"text\"].split()\n",
        "  best_sequence = dp_min_perplexity(scrambled_words)\n",
        "  predictions.append(best_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKAepFbTTiUj",
        "outputId": "4146c171-333a-4849-f673-1cdc9175822c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 48min 7s, sys: 17.9 s, total: 48min 25s\n",
            "Wall time: 49min 2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "predictions = []\n",
        "for i, row in train_df.iterrows():\n",
        "    scrambled_words = row[\"text\"].split()\n",
        "    best_sequence = dp_min_perplexity(scrambled_words)\n",
        "    predictions.append(best_sequence)"
      ],
      "metadata": {
        "id": "VanWT-RCVnJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create the submission file\n",
        "%%time\n",
        "submission_df = pd.DataFrame({\n",
        "    \"id\": train_df[\"id\"],\n",
        "    \"predicted_text\": predictions\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Submission file created: submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YynPRGpUEON",
        "outputId": "3cd9707b-92b9-422c-bf31-8a749cbe4ef8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file created: submission.csv\n",
            "CPU times: user 3.62 ms, sys: 0 ns, total: 3.62 ms\n",
            "Wall time: 3.91 ms\n"
          ]
        }
      ]
    }
  ]
}